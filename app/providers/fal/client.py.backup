from __future__ import annotations

import logging
import time
import threading

import httpx
import requests
from loguru import logger

from app.core.config import settings

DEFAULT_TIMEOUT = 60.0
DOWNLOAD_MAX_ATTEMPTS = 1  # Single attempt - if it fails, send by URL instead
DOWNLOAD_CONNECT_TIMEOUT = 10.0  # Reasonable timeout for SSL handshake
DOWNLOAD_READ_TIMEOUT = 300.0  # 5 minutes for large upscale files (15MB+)
DOWNLOAD_RETRY_BACKOFF = 1.5
RUN_BASE_URL = "https://fal.run"
RUN_CONNECT_TIMEOUT = 60.0
RUN_READ_TIMEOUT = 150.0
RUN_WRITE_TIMEOUT = 60.0
RUN_MAX_ATTEMPTS = 3
RUN_RETRY_BACKOFF = 2.0

# Global HTTP client with connection pooling for better performance
_http_client_lock = threading.Lock()
_http_client: httpx.Client | None = None
_http_client_timeout = httpx.Timeout(
    connect=60.0,
    read=60.0,
    write=30.0,
    pool=30.0,
)
_http_client_limits = httpx.Limits(
    max_keepalive_connections=20,
    max_connections=100,
    keepalive_expiry=30.0,
)


def _get_http_client() -> httpx.Client:
    """Get or create global HTTP client with connection pooling."""
    global _http_client
    if _http_client is None:
        with _http_client_lock:
            if _http_client is None:
                _http_client = httpx.Client(
                    timeout=_http_client_timeout,
                    limits=_http_client_limits,
                )
    return _http_client


def _close_http_client() -> None:
    """Close global HTTP client (for cleanup)."""
    global _http_client
    if _http_client is not None:
        with _http_client_lock:
            if _http_client is not None:
                _http_client.close()
                _http_client = None


def _normalize_path(path: str) -> str:
    return path.strip("/")


def _build_queue_url(model: str, suffix: str | None = None) -> str:
    base = settings.fal_queue_base_url.rstrip("/")
    model_path = _normalize_path(model)
    if suffix:
        return f"{base}/{model_path}/{_normalize_path(suffix)}"
    return f"{base}/{model_path}"


QUEUE_BASE_OVERRIDES = {
    "fal-ai/recraft/upscale/crisp": "fal-ai/recraft/upscale/crisp",
    "fal-ai/recraft/upscale/creative": "fal-ai/recraft/upscale/creative",
    "fal-ai/esrgan": "fal-ai/esrgan",
}

for override_candidate in (
    getattr(settings, "fal_upscale_model", None),
    getattr(settings, "fal_upscale_fallback_model", None),
):
    if override_candidate:
        normalized_candidate = override_candidate.strip("/")
        if normalized_candidate.startswith("fal-ai/recraft/upscale/") or normalized_candidate.startswith("fal-ai/esrgan"):
            QUEUE_BASE_OVERRIDES[normalized_candidate] = normalized_candidate


def _base_model_path(model: str) -> str:
    normalized = _normalize_path(model)
    override = QUEUE_BASE_OVERRIDES.get(normalized)
    if override:
        return override
    parts = normalized.split("/")
    if len(parts) > 2:
        return "/".join(parts[:-1])
    return normalized


def _get_headers(content_type: bool = True) -> dict[str, str]:
    headers = {"Authorization": f"Key {settings.fal_api_key}"}
    if content_type:
        headers["Content-Type"] = "application/json"
    return headers


def _log_request(method: str, url: str, payload: dict | None) -> None:
    logger.debug("fal request: {} {} payload={}", method, url, payload)


def _log_response(url: str, status: int, data: dict | str) -> None:
    logger.debug("fal response: {} {} -> {}", url, status, data)


def queue_submit(model: str, payload: dict) -> dict:
    url = _build_queue_url(model)
    _log_request("POST", url, payload)
    client = _get_http_client()
    response = client.post(url, json=payload, headers=_get_headers())
    if response.is_error:
        _log_response(url, response.status_code, response.text)
        response.raise_for_status()
    data = response.json()
    _log_response(url, response.status_code, data)
    return data


def run_model(model: str, payload: dict) -> dict:
    model_path = _normalize_path(model)
    url = f"{RUN_BASE_URL}/{model_path}"
    _log_request("POST", url, payload)
    timeout = httpx.Timeout(
        connect=RUN_CONNECT_TIMEOUT,
        read=RUN_READ_TIMEOUT,
        write=RUN_WRITE_TIMEOUT,
        pool=30.0,
    )
    last_error: Exception | None = None
    for attempt in range(RUN_MAX_ATTEMPTS):
        try:
            # Use global client for connection pooling, but with custom timeout
            client = _get_http_client()
            response = client.post(url, json=payload, headers=_get_headers(), timeout=timeout)
            if response.is_error:
                _log_response(url, response.status_code, response.text)
                response.raise_for_status()
            data = response.json()
            _log_response(url, response.status_code, data)
            return data
        except (httpx.TimeoutException, httpx.ReadTimeout, httpx.RemoteProtocolError) as exc:
            last_error = exc
            logger.warning(
                "Attempt {}: run_model {} timed out: {}",
                attempt + 1,
                model,
                exc,
            )
            if attempt < RUN_MAX_ATTEMPTS - 1:
                time.sleep(RUN_RETRY_BACKOFF * (attempt + 1))
        except httpx.HTTPError:
            raise
        except Exception as exc:  # noqa: BLE001
            last_error = exc
            logger.warning("Attempt {}: run_model {} failed: {}", attempt + 1, model, exc)
            if attempt < RUN_MAX_ATTEMPTS - 1:
                time.sleep(RUN_RETRY_BACKOFF * (attempt + 1))
    if last_error:
        raise last_error
    raise RuntimeError("run_model failed without explicit error")


def queue_get(url: str) -> dict:
    _log_request("GET", url, None)
    client = _get_http_client()
    response = client.get(url, headers=_get_headers(content_type=False))
    if response.is_error:
        _log_response(url, response.status_code, response.text)
        response.raise_for_status()
    data = response.json()
    _log_response(url, response.status_code, data)
    return data


def queue_status(model: str, request_id: str) -> dict:
    base_model = _base_model_path(model)
    url = _build_queue_url(base_model, f"requests/{request_id}/status")
    try:
        return queue_get(url)
    except httpx.HTTPStatusError as exc:
        normalized = _normalize_path(model)
        if exc.response.status_code == 404 and base_model != normalized:
            fallback_url = _build_queue_url(normalized, f"requests/{request_id}/status")
            return queue_get(fallback_url)
        raise


def queue_result(model: str, request_id: str) -> dict:
    base_model = _base_model_path(model)
    normalized = _normalize_path(model)
    candidate_paths: list[str] = [
        _build_queue_url(base_model, f"requests/{request_id}"),
    ]
    if base_model != normalized:
        candidate_paths.append(_build_queue_url(normalized, f"requests/{request_id}"))
    candidate_paths.append(_build_queue_url(base_model, f"requests/{request_id}/response"))
    candidate_paths.append(_build_queue_url(base_model, f"requests/{request_id}/result"))
    if base_model != normalized:
        candidate_paths.append(_build_queue_url(normalized, f"requests/{request_id}/response"))
        candidate_paths.append(_build_queue_url(normalized, f"requests/{request_id}/result"))

    # Retry logic for server errors (500, 502, 503)
    max_retries = 3
    retry_delay = 1.0
    last_error: httpx.HTTPStatusError | None = None
    
    for attempt in range(max_retries):
        last_error = None
        for candidate in candidate_paths:
            try:
                result = queue_get(candidate)
                logger.debug("queue_result {} succeeded on attempt {} with candidate {}", request_id, attempt + 1, candidate)
                return result
            except httpx.HTTPStatusError as exc:
                last_error = exc
                status_code = exc.response.status_code
                logger.debug("queue_result {} -> {} {}", candidate, status_code, exc.response.text[:100] if hasattr(exc.response, 'text') else str(exc))
                
                # Retry on server errors (500, 502, 503) and auth errors (401) - may be temporary
                if status_code in (500, 502, 503, 401):
                    if attempt < max_retries - 1:
                        logger.warning(
                            "queue_result {} attempt {} failed with {}: {}. Retrying all candidates in {:.1f}s",
                            request_id,
                            attempt + 1,
                            status_code,
                            exc.response.text[:100] if hasattr(exc.response, 'text') else str(exc),
                            retry_delay,
                        )
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        break  # Retry from first candidate
                    else:
                        # Last attempt failed, raise
                        logger.error("queue_result {} failed after {} attempts with {}", request_id, max_retries, status_code)
                        raise
                # Don't retry on client errors (404, 405, 422) - try next candidate
                elif status_code in (404, 405, 422):
                    logger.debug("queue_result {} -> {} (trying next candidate)", candidate, status_code)
                    continue  # Try next candidate
                else:
                    # Other errors - raise immediately
                    logger.error("queue_result {} -> {} (non-retryable)", candidate, status_code)
                    raise
        
        # If we got here and last_error is None, we succeeded (shouldn't happen, but just in case)
        if last_error is None:
            break
    
    if last_error:
        raise last_error
    raise RuntimeError("queue_result failed without HTTP error")


def download_file(url: str, target_path: str) -> None:
    import time as time_module
    start_time = time_module.time()
    logger.debug("Downloading fal result from {} to {}", url, target_path)
    last_error: Exception | None = None
    for attempt in range(DOWNLOAD_MAX_ATTEMPTS):
        try:
            attempt_start = time_module.time()
            logger.info("Download attempt {}/{}: starting request to {} (timeout: connect={}s, read={}s)", 
                       attempt + 1, DOWNLOAD_MAX_ATTEMPTS, url, DOWNLOAD_CONNECT_TIMEOUT, DOWNLOAD_READ_TIMEOUT)
            
            # Use requests with proper SSL configuration
            # Disable SSL verification warnings but keep verification enabled
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Connection': 'keep-alive'
            })
            
            # Use tuple (connect_timeout, read_timeout) for requests
            timeout_tuple = (DOWNLOAD_CONNECT_TIMEOUT, DOWNLOAD_READ_TIMEOUT)
            
            request_start = time_module.time()
            
            # Try HEAD request first to get file size, but don't fail if it doesn't work
            # Use shorter timeout for HEAD (5s) - if it fails, skip HEAD and go straight to GET
            file_size_mb = None
            try:
                head_timeout = (DOWNLOAD_CONNECT_TIMEOUT, 5.0)  # Short timeout for HEAD
                head_response = session.head(url, timeout=head_timeout, allow_redirects=True)
                content_length = head_response.headers.get("content-length")
                if content_length:
                    file_size_mb = int(content_length) / (1024 * 1024)
                    logger.info("Download attempt {}: detected file size {:.2f}MB from HEAD request", 
                               attempt + 1, file_size_mb)
            except Exception as head_exc:
                logger.debug("Download attempt {}: HEAD request failed (will proceed with GET): {}", 
                           attempt + 1, head_exc)
            
            # Use streaming download for all files to avoid memory issues
            logger.info("Download attempt {}: starting streaming download", attempt + 1)
            response = session.get(url, stream=True, timeout=timeout_tuple, allow_redirects=True)
            response.raise_for_status()
            
            content_length_downloaded = 0
            with open(target_path, "wb") as file:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        file.write(chunk)
                        content_length_downloaded += len(chunk)
            
            content_size = content_length_downloaded
            total_time = time_module.time() - attempt_start
            logger.info("Download attempt {}: saved {} bytes ({:.2f}MB) to {} in {:.2f}s total", 
                       attempt + 1, content_size, content_size / (1024 * 1024), target_path, total_time)
            session.close()
            return
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, requests.exceptions.SSLError) as exc:
            last_error = exc
            logger.warning(
                "Attempt {}: failed to download result from {}: {}",
                attempt + 1,
                url,
                exc,
            )
            if attempt < DOWNLOAD_MAX_ATTEMPTS - 1:
                sleep_time = DOWNLOAD_RETRY_BACKOFF * (attempt + 1)
                logger.debug("Waiting {} seconds before retry...", sleep_time)
                time.sleep(sleep_time)
        except Exception as exc:
            last_error = exc
            logger.error("Unexpected error during download attempt {}: {}", attempt + 1, exc, exc_info=True)
            if attempt < DOWNLOAD_MAX_ATTEMPTS - 1:
                sleep_time = DOWNLOAD_RETRY_BACKOFF * (attempt + 1)
                logger.debug("Waiting {} seconds before retry...", sleep_time)
                time.sleep(sleep_time)
    if last_error:
        logger.error("All download attempts failed for {}: {}", url, last_error)
        raise last_error
